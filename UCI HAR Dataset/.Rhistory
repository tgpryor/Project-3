## [1,]   -2  1.5
## [2,]    1 -0.5
## Create a special "matrix", which is a list containing a function to
##   - set the value of the matrix
##   - get the value of the matrix
##   - set the value of the cache (inverse matrix)
##   - get the value of the cache (inverse matrix)
# MAKECACHEMATRIX() - store and retrieve a matrix, store and retrieve a cache
makeCacheMatrix <- function(x = numeric()) {
# Initialize cache to NULL
cache <- NULL
# SETMATRIX()
setMatrix <- function(newValue) {
# Store numeric matrix
x <<- newValue
# Flush the cache
cache <<- NULL
}
# GETMATRIX()
getMatrix <- function() {
# Return the stored matrix
x
}
# SETCACHE()
setCache <- function(solve) {
# Cache the matrix
cache <<- solve
}
# GETCACHE()
getCache <- function() {
# Return the cached matrix
cache
}
# Return a list: each named element is a function which can subsequently be called
list(setMatrix=setMatrix, getMatrix=getMatrix, setCache=setCache, getCache=getCache)
}
# CACHESOLVE() - return the inverse of the matrix created by MAKECACHEMATRIX. Inverse values are only calculated once and
# cached.  Subsequent calls will simply return the cache.
cacheSolve <- function(y, ...) {
# Get the cache
inverse <- y$getCache()
# If the cache value exists (is not NULL), return it
if(!is.null(inverse)) {
message("getting cached data")
return(inverse)
}
# Since the cache is empty, calculate the inverse
data <- y$getMatrix()
inverse <- solve(data)
# Cache the matrix of inverse values
y$setCache(inverse)
# Return the inverse values
inverse
}
m <- makeCacheMatrix(matrix(c(1, 2, 3, 4), nrow=2, ncol=2)
)
## Functions that cache the inverse of a matrix
##
## Usage example:
##
## > source('cachematrix.R')
## > m <- makeCacheMatrix(matrix(c(1, 2, 3, 4), nrow=2, ncol=2))
## > m$getMatrix()
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
## > cacheSolve(m)
##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5
## > m$getInverse()
##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5
## > cacheSolve(m)
## getting cached data
##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5
## Create a special "matrix", which is a list containing a function to
##   - set the value of the matrix
##   - get the value of the matrix
##   - set the value of the cache (inverse matrix)
##   - get the value of the cache (inverse matrix)
# MAKECACHEMATRIX() - store and retrieve a matrix, store and retrieve a cache
makeCacheMatrix <- function(x = numeric()) {
# Initialize cache to NULL
cache <- NULL
# SETMATRIX()
setMatrix <- function(newValue) {
# Store numeric matrix
x <<- newValue
# Flush the cache
cache <<- NULL
}
# GETMATRIX()
getMatrix <- function() {
# Return the stored matrix
x
}
# SETCACHE()
setCache <- function(solve) {
# Cache the matrix
cache <<- solve
}
# GETCACHE()
getCache <- function() {
# Return the cached matrix
cache
}
# Return a list: each named element is a function which can subsequently be called
list(setMatrix=setMatrix, getMatrix=getMatrix, setCache=setCache, getCache=getCache)
}
# CACHESOLVE() - return the inverse of the matrix created by MAKECACHEMATRIX. Inverse values are only calculated once and
# cached.  Subsequent calls will simply return the cache.
cacheSolve <- function(y, ...) {
# Get the cache
inverse <- y$getCache()
# If the cache value exists (is not NULL), return it
if(!is.null(inverse)) {
message("getting cached data")
return(inverse)
}
# Since the cache is empty, calculate the inverse
data <- y$getMatrix()
inverse <- solve(data)
# Cache the matrix of inverse values
y$setCache(inverse)
# Return the inverse values
inverse
}
m <- makeCacheMatrix(matrix(c(1, 2, 3, 4), nrow=2, ncol=2))
m$getMatrix()
cacheSolve(m)
m$getCache()
cacheSolve(m)
## Functions that cache the inverse of a matrix
##
## Usage example:
##
## > source('cachematrix.R')
## > m <- makeCacheMatrix(matrix(c(1, 2, 3, 4), nrow=2, ncol=2))
## > m$getMatrix()
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
## > cacheSolve(m)
##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5
## > m$getCache()
##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5
## > cacheSolve(m)
## getting cached data
##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5
## Create a special "matrix", which is a list containing a function to
##   - set the value of the matrix
##   - get the value of the matrix
##   - set the value of the cache (inverse matrix)
##   - get the value of the cache (inverse matrix)
# MAKECACHEMATRIX() - store and retrieve a matrix, store and retrieve a cache
makeCacheMatrix <- function(x = matrix()) {
# Initialize cache to NULL
cache <- NULL
# SETMATRIX()
setMatrix <- function(newValue) {
# Store numeric matrix
x <<- newValue
# Flush the cache
cache <<- NULL
}
# GETMATRIX()
getMatrix <- function() {
# Return the stored matrix
x
}
# SETCACHE()
setCache <- function(solve) {
# Cache the matrix
cache <<- solve
}
# GETCACHE()
getCache <- function() {
# Return the cached matrix
cache
}
# Return a list: each named element is a function which can subsequently be called
list(setMatrix=setMatrix, getMatrix=getMatrix, setCache=setCache, getCache=getCache)
}
# CACHESOLVE() - return the inverse of the matrix created by MAKECACHEMATRIX. Inverse values are only calculated once and
# cached.  Subsequent calls will simply return the cache.
cacheSolve <- function(x, ...) {
# Get the cache
inverse <- x$getCache()
# If the cache value exists (is not NULL), return it
if(!is.null(inverse)) {
message("getting cached data")
return(inverse)
}
# Since the cache is empty, calculate the inverse
data <- x$getMatrix()
inverse <- solve(data)
# Cache the matrix of inverse values
x$setCache(inverse)
# Return the inverse values
inverse
}
m <- makeCacheMatrix(matrix(c(1, 2, 3, 4), nrow=2, ncol=2))
m$getMatrix()
cacheSolve(m)
m$getCache()
cacheSolve(m)
library(datasets)
data("iris")
?iris
iris
virginica <- iris$Species=="virginica"
virginica
iris[iris$species=="virginica"]
iris
iris[iris$species=="virginica",]
virginica <- iris[iris$species=="virginica",]
virginica
mean(iris[iris$species=="virginica",]$Sepal.Length)
data("iris")
?iris
mean(iris[iris$Species=="virginica",]$Sepal.Length)
colMeans(iris)
apply(iris[, 1:4],2, mean)
apply(iris, 1, mean)
apply(iris[, 1:4], 1,, mean)
library(datasets)
data(mdcars)
data("mtcars")
?mtcars
mean(mtcars$mpg, mtcars$cyl)
tapply(mtcars$mpg, mtcars$cyl, mean)
mean(mtcars[mtcars$cyl == "8",]$hp) - mean(mtcars[mtcars$cyl == "4",]$hp)
install.packages("XML")
library(XML)
doc <- xmlTreeParse("https://itunes.apple.com/us/rss/topsongs/limit=25/explicit=true/xml")
? xmlTreeParse
library("twitteR")
install.packages("twitter")
install.packages("twitteR")
library("twitteR")
install.packages("reshape2")
library("reshape2")
install.packages("ggplot2")
library(ggplot2)
install.packages("plyr")
doc <- searchTwitteR("Romney", n=25)
getTwitterOAuth("8viDUg7tTJBL3IWaGZE3w3jPf", "XgAVBB38FbC2cC8HXnSlbVUc4ZTbuHsqvxeeHQkVAHz5uwukz9")
consumerKey <- "8viDUg7tTJBL3IWaGZE3w3jPf"
consumerSecret <- "XgAVBB38FbC2cC8HXnSlbVUc4ZTbuHsqvxeeHQkVAHz5uwukz9"
getTwitterOAuth(consumerKey, consumerSecret)
?setup_twitter_oauth
setup_twitter_oauth(consumerKey, consumerSecret)
setup_twitter_oauth(consumerKey, consumerSecret)
install.packages("base64enc")
library(base64enc)
setup_twitter_oauth(consumerKey, consumerSecret)
setup_twitter_oauth("CONSUMER_KEY", "CONSUMER_SECRET")
getTwitterOAuth(consumer_key = consumerKey, consumer_secret = consumerSecret)
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
accessToken <- "3196482764-b3vcXxW222y9ExpHINHaTs0nSJk3O84duixyRW7"
accessSecret <- "LeuqXlRUDb2YVrLGgNiZZ3ZZYDaESQ8dbAvgdTCiCDhRY"
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
library(twitteR)
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
install.packages("xlsx")
library(xlsx)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileURL, destfile="gas.xlsx", method="curl")
data <- read.xlsx(gas.xlsx, sheetIndex=1, header=TRUE)
head(data)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileURL, destfile="gas.xlsx", method="curl")
data <- read.xlsx(gas.xlsx, sheetIndex=1, header=TRUE)
head(data)
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx(gas.xlsx, sheetIndex=1, colIndex=colIndex, rowIndex = rowIndex)
dat
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx("gas.xlsx", sheetIndex=1, colIndex=colIndex, rowIndex = rowIndex)
dat
sum(dat$Zip*dat$Ext,na.rm=T)
install.packages("XML")
library(XML)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
download.file(fileURL, destfile="restaurants.xml", method="curl")
doc <- xmlTreeParse(fileURL, useInternal=TRUE)
doc <- xmlTreeParse(fileURL)
doc <- xmlTreeParse(fileURL, useInternalNodes=TRUE)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileURL, useInternalNodes=TRUE)
doc <- xmlParse(fileURL)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata/data/restaurants.xml"
doc <- xmlTreeParse(fileURL, useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
clear
cls
fileURL <- "http://d396qusza40orc.cloudfront.net/getdata/data/restaurants.xml"
doc <- xmlTreeParse(fileURL, useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[[1]][[1]]
zipcode <- xpathSApply(rootNode, "//zipcode", xmlValue)
zipcode
zipcode[zipcode==21231]
nrow(zipcode[zipcode==21231])
length(zipcode[zipcode==21231])
library(data.table)
install.packages("data.table")
library(data.table)
DT <- fread(input="idaho.csv", sep=",")
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
rowMeans(DT)[DT$SEX==1]
rowMeans(DT)[DT$SEX==2]
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile="idaho.csv", mode="w" method="curl")
DT <- fread(input="idaho.csv", sep=",")
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile="idaho.csv", mode="w", method="curl")
DT <- fread(input="idaho.csv", sep=",")
rowMeans(DT)[DT$SEX==1]
rowMeans(DT)[DT$SEX==2]
tapply(DT$pwgtp15,DT$SEX,mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
mean(DT$pwgtp15,by=DT$SEX)
DT[,mean(pwgtp15),by=SEX]
sapply(split(DT$pwgtp15,DT$SEX),mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
# system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
# mean(DT$pwgtp15,by=DT$SEX)
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
library(httr)
oauth_endpoints("github")
#
myapp <- oauth_app("github", "ClientID", "ClientSecret")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", "a06305fc453f233476bd", "6e6b173d8c0c26e5487b344e637853ba2d0d90cf")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("github")
#
myapp <- oauth_app("github", "a06305fc453f233476bd", "6e6b173d8c0c26e5487b344e637853ba2d0d90cf")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("github")
#
myapp <- oauth_app("github", "a06305fc453f233476bd", "6e6b173d8c0c26e5487b344e637853ba2d0d90cf")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("Development App")
#
myapp <- oauth_app("Development App", "a06305fc453f233476bd", "6e6b173d8c0c26e5487b344e637853ba2d0d90cf")
github_token <- oauth2.0_token(oauth_endpoints("Development App"), myapp)
library(httr)
# Find OAuth setting for github
oauth_endpoints("github")
# To make your own application, register at at
# https://github.com/settings/applications. Use any URL for the homepage URL
# (http://github.com is fine) and http://localhost:1410 as the callback url
# Use my own key and secret below.
myapp <- oauth_app("Development App",
key="a06305fc453f233476bd",
secret="6e6b173d8c0c26e5487b344e637853ba2d0d90cf")
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token=github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
# curl -u Access Token:x-oauth-basic "https://api.github.com/users/jtleek/repos"
BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
# 2013-11-07T13:25:07Z
install.packages("sqldf")
library(sqldf)
acs <- read.csv("https://d396qusza40orc.cloudfront.net/getdata/data/ss06pid.csv", header=T, sep=",")
head(acs)
sqldf("select")
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select distinct AGEP from acs")
length(unique(acs$AGEP))
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
con <- url(url)
htmlCode <- readLines(con)
close(con)
sapply(htmlCode[c(10, 20, 30, 100)], nchar)
data <- read.csv("getdata_wksst8110.for", header=T)
head(data)
dim(data)
file_name <- "getdata_wksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(df)
sum(df[, 4])
data <- read.csv("https://d396qusza40orc.cloudfront.net/getdata/wksst8110.for", header=T)
head(data)
dim(data)
file_name <- "getdata_wksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(df)
sum(df[, 4])
data <- read.csv("https://d396qusza40orc.cloudfront.net/getdata/wksst8110.for", header=T)
head(data)
dim(data)
file_name <- "https://d396qusza40orc.cloudfront.net/getdata/wksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(df)
sum(df[, 4])
##########################################################################################################
# Coursera Getting and Cleaning Data Course Project
# Travis Pryor
# 08-23-2015
# The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set.
# The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers
# on a series of yes/no questions related to the project. You will be required to submit:
# 1) a tidy data set as described below,
# 2) a link to a Github repository with your script for performing the analysis, and
# 3) a code book that describes the variables, the data, and any transformations or work that you
#    performed to clean up the data called CodeBook.md. You should also include a README.md in the repo
#    with your scripts. This repo explains how all of the scripts work and how they are connected.
#
# One of the most exciting areas in all of data science right now is wearable computing. Companies like
# Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users.
# The data linked to from the course website represent data collected from the accelerometers from the
# Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
# - http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
# Here are the data for the project:
# - https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles/UCI HAR Dataset.zip
#
# You should create one R script called run_analysis.R that does the following.
# - Merges the training and the test sets to create one data set.
# - Extracts only the measurements on the mean and standard deviation for each measurement.
# - Uses descriptive activity names to name the activities in the data set
# - Appropriately labels the data set with descriptive variable names.
# - From the data set in step 4, creates a second, independent tidy data set with the average of each
#   variable for each activity and each subject.
##########################################################################################################
# Clean up workspace
rm(list=ls())
#set working directory to the location where the UCI HAR Dataset was unzipped
setwd('/Users/travis/Coursera/2 Getting and Cleaning Data/Course-Project/UCI HAR Dataset/');
# LOAD DATA FROM UNZIPPED FLAT FILES
# activity
dataActivityTest  <- read.table(file.path("./test/Y_test.txt" ), header = FALSE)
dataActivityTrain <- read.table(file.path("./train/Y_train.txt"), header = FALSE)
# subjects
dataSubjectTest  <- read.table(file.path("./test/subject_test.txt"), header = FALSE)
dataSubjectTrain <- read.table(file.path("./train/subject_train.txt"), header = FALSE)
# features
dataFeaturesTest  <- read.table(file.path("./test/X_test.txt"), header = FALSE)
dataFeaturesTrain <- read.table(file.path("./train/X_train.txt"), header = FALSE)
# MERGE TRAINING AND TEST DATA INTO ONE DATASET
# concatenate rows
dataSubject    <- rbind(dataSubjectTrain, dataSubjectTest)
dataActivity   <- rbind(dataActivityTrain, dataActivityTest)
dataFeatures   <- rbind(dataFeaturesTrain, dataFeaturesTest)
# name variables
names(dataSubject)  <- c("subject")
names(dataActivity) <- c("activity")
dataFeaturesNames   <- read.table(file.path("features.txt"), head=FALSE)
names(dataFeatures) <- dataFeaturesNames$V2
# create master dataset with necessary columns
dataCombine    <- cbind(dataSubject, dataActivity)
Data           <- cbind(dataFeatures, dataCombine)
# ONLY KEEP THE MEAN AND STDEV FOR EACH MEASUREMENT
# subset features, looking for "mean()" or "std()"
subdataFeaturesNames <- dataFeaturesNames$V2[grep("mean\\(\\)|std\\(\\)", dataFeaturesNames$V2)]
# subset master dataset with selected features
selectedNames  <- c(as.character(subdataFeaturesNames), "subject", "activity" )
Data           <- subset(Data,select=selectedNames)
# APPLY DESCRIPTIVE ACTIVITY NAMES
# load activity names
activityLabels <- read.table(file.path("activity_labels.txt"), header=FALSE)
# apply labels to data values
Data$activity = activityLabels[,2][match(Data$activity, activityLabels[,1])]
# RENAME VARIABLES
# add descriptive variable names
names(Data) <- gsub("^t", "time", names(Data))
names(Data) <- gsub("^f", "frequency", names(Data))
names(Data) <- gsub("Acc", "Accelerometer", names(Data))
names(Data) <- gsub("Gyro", "Gyroscope", names(Data))
names(Data) <- gsub("Mag", "Magnitude", names(Data))
names(Data) <- gsub("BodyBody", "Body", names(Data))
# CREATE TIDY DATASET
# convert master dataset to flat file
#library(plyr);
Data2 <- aggregate(. ~subject + activity, Data, mean)
Data2 <- Data2[order(Data2$subject,Data2$activity),]
write.table(Data2, file="tidydata.txt", row.name=FALSE)
